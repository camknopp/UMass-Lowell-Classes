{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Cameron Knopp\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this homework you will see what [word embeddings](https://en.wikipedia.org/wiki/Word_embedding) are, how to train them and how to evaluate them. Word embeddings are widely used in practice and you will most likely use them in your projects. Keep in mind that training a neural network can take time, so before running the notebook please test your implementation on a smaller subset of your dataset.\n",
    "\n",
    "__You will learn:__\n",
    "- To implement your own neural network that trains word embeddings (using the code from the previous homework)\n",
    "- Learn how to run intrinsic evaluations on the obtained embeddings\n",
    "- Compare your own embeddings to the pre-trained ones\n",
    "\n",
    "__You will need:__\n",
    "\n",
    "- To install the `gensim` package as `pip install gensim`\n",
    "- Access to GPUs\n",
    "- The code from your second homework\n",
    "\n",
    "Once you complete this assignment, submit it as:\n",
    "\n",
    "`submit arum hw3 <name_of_your_notebook>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that execution of this cell doesn't return any errors. If it does, go the class repository and follow the environment setup instructions\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk import word_tokenize\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training a skip-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skip-gram model is a subtype of the [Word2Vec model ](https://en.wikipedia.org/wiki/Word2vec) that aims to predict context words given a target word. In the previous homework, you implemented all the necessary classes to feed the data in the skip-gram model. In this step, you will implement the model architecture and the training loop. You will then put everything together to train the skip-gram model, save the trained word embeddings, and evaluate them.\n",
    "\n",
    "The high-level overview of this part of the homework is as follows:\n",
    "\n",
    "- Complete the `SkipGramModel` class:\n",
    "    * Initialize the layers\n",
    "    * Complete the `forward()` method\n",
    "    * Complete the `save_embeddings()` method\n",
    "- Prepare the data. Use the code from the previous homework to:\n",
    "    * Tokenize the input text\n",
    "    * Construct and prune the vocabulary\n",
    "    * Initialize the `SkipGramDataset` class\n",
    "    * Initialize the `DataLoader` class\n",
    "    * Initialize the model class\n",
    "- Complete the training loop:\n",
    "    * Fetch the input batches from the dataloader\n",
    "    * Run the inputs through the model and get the predictions\n",
    "    * Calculate the loss\n",
    "    * Backpropagate the error\n",
    "    * Update the weights\n",
    "    * Save the embeddings\n",
    "- Visualize the learning curve\n",
    "\n",
    "__Insert the code for the `preprocess()` function, the `Vocabulary` class, and the `SkipGramDataset` class in the corresponding cells below and execute those cells. Follow the instructions below to complete this part of the homework.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (str):\n",
    "    Returns: a list of tokens\n",
    "\n",
    "    \"\"\"\n",
    "    ### YOUR CODE BELOW ###\n",
    "    tokens_with_punct = word_tokenize(data)\n",
    "    tokens = [token for token in tokens_with_punct if token.isalpha()] # remove punctuation from tokens\n",
    "    ### YOUR CODE ABOVE ###\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, special_tokens=None):\n",
    "        self.w2idx = {}\n",
    "        self.idx2w = {}\n",
    "        self.w2cnt = defaultdict(int)\n",
    "        self.special_tokens = special_tokens\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_tokens(special_tokens)\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        for token in tokens:\n",
    "            self.add_token(token)\n",
    "            self.w2cnt[token] += 1\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.w2idx:\n",
    "            cur_len = len(self)\n",
    "            self.w2idx[token] = cur_len\n",
    "            self.idx2w[cur_len] = token\n",
    "\n",
    "    def prune(self, min_cnt=2):\n",
    "        to_remove = set([token for token in self.w2idx if self.w2cnt[token] < min_cnt])\n",
    "        if self.special_tokens is not None:\n",
    "            to_remove = to_remove.difference(set(self.special_tokens))\n",
    "        \n",
    "        for token in to_remove:\n",
    "            self.w2cnt.pop(token)\n",
    "            \n",
    "        self.w2idx = {token: idx for idx, token in enumerate(self.w2cnt.keys())}\n",
    "        self.idx2w = {idx: token for token, idx in self.w2idx.items()}\n",
    "    \n",
    "    def __contains__(self, item):\n",
    "        return item in self.w2idx\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return self.w2idx[item]\n",
    "        elif isinstance(item , int):\n",
    "            return self.idx2w[item]\n",
    "        else:\n",
    "            raise TypeError(\"Supported indices are int and str\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.w2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, data, vocab, skip_window=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        self.skip_window = skip_window\n",
    "\n",
    "        self.pairs = self._generate_pairs(data, skip_window)\n",
    "\n",
    "    def _generate_pairs(self, data, skip_window):\n",
    "        \"\"\"\n",
    "        Args: input data (a list of tokens)\n",
    "        Returns: all possible pairs for the SkipGram mode\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            for j in range(-skip_window, skip_window + 1):\n",
    "                context_idx = i + j\n",
    "                if j == 0 or context_idx < 0 or context_idx >= len(data):\n",
    "                    continue\n",
    "                if data[i] not in self.vocab or data[context_idx] not in self.vocab:\n",
    "                    continue\n",
    "                pairs.append((data[i], data[context_idx]))\n",
    "        return pairs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        pair = self.pairs[idx]\n",
    "        pair = [self.vocab[t] for t in pair]\n",
    "        return pair\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        \"\"\"\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the models implemented in PyTorch should subclass the [`torch.nn.Module` class](https://pytorch.org/docs/stable/nn.html?highlight=module#torch.nn.Module). The main method of this class (which is used by a lot of other PyTorch classes) is `forward()`. `forward()` is the core method that defines how your model is going to run and what outputs it should produce given the inputs. \n",
    "In the constructor of the your model class you should initialize all the layers you are going to use. The skip-gram model is conceptually simple - it stores embeddings in a trainable matrix and projects the target word's embedding to a vector of the vocabulary size for making a prediction about the context word. Your task is to complete the code in the `SkipGramModel` class below. Please refer to the [documentation of PyTorch](https://pytorch.org/docs/stable/nn.html) to correctly initialize the layers.\n",
    "\n",
    "__Instructions__:\n",
    "\n",
    "- Complete the class constructor\n",
    "    * Initialize the `nn.Embedding` layer\n",
    "    * Initialize the `nn.Linear` projection layer\n",
    "- Complete the `forward()` method that takes a tensor of the shape [Bx1] and returns a prediction vector of the shape [BxV], where B is the batch size and V is the vocabulary size.\n",
    "- Complete the `save_embeddings()` method that extracts the weights of the embedding layers and saves them to a file. The format of the file should be as follows:\n",
    "\n",
    "```\n",
    "<number_of_words> <dimension>\n",
    "<word1>                <num_1> <num_2> ... <num_dimension>\n",
    "<word2>                <num_1> <num_2> ... <num_dimension>\n",
    "...\n",
    "<word_number_of_words> <num_1> <num_2> ... <num_dimension>\n",
    "```\n",
    "For more details, please refer to the section 2 of this notebook.\n",
    "\n",
    "__Notes__:\n",
    "\n",
    "- The weights of a layer can be extracted using the `weight` attribute.\n",
    "- To convert `torch.Tensor` to a `numpy` array you first need to detach it from the current computational graph and then transfer it to CPU, which can be done using a chain of the `.detach()` and the `.cpu()` methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SkipGramModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): vocabulary size\n",
    "            embedding_dim (int): the dimension of word embeddings\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        #self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.linear = torch.nn.Linear(1, vocab_size)\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the skip-gram model.\n",
    "        \n",
    "        Args:\n",
    "            inputs (torch.LongTensor): input tensor containing batches of word ids [Bx1]\n",
    "        Returns:\n",
    "            outputs (torch.FloatTensor): output tensor with unnormalized probabilities over the vocabulary [BxV]\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeds = self.embedding(inputs)\n",
    "        #embeds = self.embedding(inputs)\n",
    "        outputs = self.linear(embeds)\n",
    "        outputs=outputs\n",
    "        #output = F.log_softmax(self.linear(embeds), dim=1)\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        return outputs\n",
    "    \n",
    "    def save_embeddings(self, voc, path):\n",
    "        \"\"\"\n",
    "        Save the embedding matrix to a specified path.\n",
    "        \n",
    "        Args:\n",
    "            voc (Vocabulary): the Vocabulary object for id-to-token mapping\n",
    "            path (str): the location of the target file\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeds = self.embedding.weight.data.cpu().numpy()\n",
    "        f = open(path, 'w')\n",
    "        f.write(str(vocab_size) + ' ' + str(embedding_dim) + '\\n')\n",
    "        \n",
    "        for idx in range(len(embeds)):\n",
    "            word = voc.idx2w[idx]\n",
    "            embedding = ' '.join(map(str,embeds[idx]))\n",
    "            f.write(word + ' '+ embedding + '\\n')\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        print(\"Successfuly saved to {}\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below reads the data, initializes all the classes and parameters necessary for training. Please carefully read through, understand, and run the code. \n",
    "\n",
    "__Notes__:\n",
    "\n",
    "- We are going to use the [CrossEntropy loss](https://en.wikipedia.org/wiki/Cross_entropy) which is a standard loss function for classification tasks. For numeric stability, [PyTorch implementation of the cross entropy function](https://pytorch.org/docs/stable/nn.html?highlight=crossentropy#torch.nn.CrossEntropyLoss) combines the log softmax function and the negative loglikelihood loss computation. Because of this, you don't need to apply the softmax function to the output of your model.\n",
    "\n",
    "- We are going to use the [Adagrad optimizer](https://pytorch.org/docs/stable/optim.html?highlight=adagrad#torch.optim.Adagrad) for gradient descent optimization.\n",
    "\n",
    "- For the sake of speed, we encourage you to keep the values of the parameters as they are. However, after you complete the assignment, feel free to change them to see what effect this would have on the model's performance (but keep in mind that training can take much longer).\n",
    "\n",
    "- It is always a good idea to shuffle the dataset (think why)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PROCESSING #\n",
    "with open('text8.txt') as f:\n",
    "    data = f.read()\n",
    "tokens = preprocess(data[:1000000])\n",
    "\n",
    "# CONSTRUCTING VOCABULARY #\n",
    "voc = Vocabulary()\n",
    "voc.add_tokens(tokens)\n",
    "voc.prune(5)\n",
    "vocab_size = len(voc)\n",
    "\n",
    "# TRAINING PARAMETERS #\n",
    "embedding_dim = 128\n",
    "skip_window = 2\n",
    "batch_size = 512\n",
    "lr = 0.1\n",
    "num_epochs = 100\n",
    "report_every = 1\n",
    "\n",
    "# DATASET\n",
    "dataset = SkipGramDataset(tokens, voc, skip_window=skip_window)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# MODEL\n",
    "model = SkipGramModel(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model and the dataset are ready, we will be looking at the training loop structure.\n",
    "Essentially, training of any neural network is an iterative process of (a) sampling a batch of training examples, (b) running them through the model, (c) computing the loss based on the predictions and the target labels, (d) performing the backward pass and computing the gradients, and (e) updating the model's weights. Keeping this logic in mind, complete the code in the cell below. \n",
    "\n",
    "__! Important__:\n",
    "Because PyTorch accumulates the gradients on subsequent backward passes, you will need to manually set the gradients to zero at every iteration to correctly update the parameters of your model. You can set the gradients to zero using the `zero_grad()` method of the used optimizer (in our case, Adagrad). \n",
    "\n",
    "__Instructions__:\n",
    "Complete the code below using the following methods:\n",
    "- [`zero_grad()`](https://pytorch.org/docs/stable/optim.html?highlight=zero_grad#torch.optim.Optimizer.zero_grad) and [`step()`](https://pytorch.org/docs/stable/optim.html?highlight=step#torch.optim.Optimizer.step) methods for optimizers\n",
    "- [`backward()`](https://pytorch.org/docs/stable/autograd.html?highlight=backward#torch.autograd.backward) method for the loss\n",
    "- [`item()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.item) method for converting a `torch.Tensor` object of the size [1] to a float number\n",
    "- `cuda()` method for transferring tensors to GPUs\n",
    "\n",
    "__Notes__:\n",
    "\n",
    "- You can always verify if your model is training correctly - the loss should be decreasing. Please keep track of your epoch losses, since you will need them to plot the learning curve later.\n",
    "\n",
    "- __Warning__: The execution of the cell below will take time depending on the selected batch size, the number of epochs and the size of the dataset. If you are not sure your implementation is correct, test it on a small subset of the dataset (you can manually reduce the dataset by cutting off a portion of the dataset right after you load it) and using a small number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss 6.3329. Elapsed 17 seconds\n",
      "Epoch 2. Loss 5.7675. Elapsed 34 seconds\n",
      "Epoch 3. Loss 5.6164. Elapsed 51 seconds\n",
      "Epoch 4. Loss 5.5247. Elapsed 68 seconds\n",
      "Epoch 5. Loss 5.4597. Elapsed 86 seconds\n",
      "Epoch 6. Loss 5.4099. Elapsed 102 seconds\n",
      "Epoch 7. Loss 5.3698. Elapsed 119 seconds\n",
      "Epoch 8. Loss 5.3365. Elapsed 136 seconds\n",
      "Epoch 9. Loss 5.3083. Elapsed 153 seconds\n",
      "Epoch 10. Loss 5.2838. Elapsed 171 seconds\n",
      "Epoch 11. Loss 5.2622. Elapsed 187 seconds\n",
      "Epoch 12. Loss 5.2432. Elapsed 205 seconds\n",
      "Epoch 13. Loss 5.2261. Elapsed 221 seconds\n",
      "Epoch 14. Loss 5.2106. Elapsed 238 seconds\n",
      "Epoch 15. Loss 5.1965. Elapsed 256 seconds\n",
      "Epoch 16. Loss 5.1836. Elapsed 273 seconds\n",
      "Epoch 17. Loss 5.1716. Elapsed 290 seconds\n",
      "Epoch 18. Loss 5.1607. Elapsed 308 seconds\n",
      "Epoch 19. Loss 5.1504. Elapsed 325 seconds\n",
      "Epoch 20. Loss 5.1409. Elapsed 342 seconds\n",
      "Epoch 21. Loss 5.1319. Elapsed 359 seconds\n",
      "Epoch 22. Loss 5.1235. Elapsed 376 seconds\n",
      "Epoch 23. Loss 5.1158. Elapsed 394 seconds\n",
      "Epoch 24. Loss 5.1081. Elapsed 411 seconds\n",
      "Epoch 25. Loss 5.1013. Elapsed 429 seconds\n",
      "Epoch 26. Loss 5.0945. Elapsed 445 seconds\n",
      "Epoch 27. Loss 5.0881. Elapsed 462 seconds\n",
      "Epoch 28. Loss 5.0822. Elapsed 479 seconds\n",
      "Epoch 29. Loss 5.0765. Elapsed 496 seconds\n",
      "Epoch 30. Loss 5.0710. Elapsed 513 seconds\n",
      "Epoch 31. Loss 5.0657. Elapsed 530 seconds\n",
      "Epoch 32. Loss 5.0607. Elapsed 548 seconds\n",
      "Epoch 33. Loss 5.0559. Elapsed 565 seconds\n",
      "Epoch 34. Loss 5.0512. Elapsed 583 seconds\n",
      "Epoch 35. Loss 5.0468. Elapsed 600 seconds\n",
      "Epoch 36. Loss 5.0426. Elapsed 617 seconds\n",
      "Epoch 37. Loss 5.0386. Elapsed 634 seconds\n",
      "Epoch 38. Loss 5.0345. Elapsed 651 seconds\n",
      "Epoch 39. Loss 5.0307. Elapsed 669 seconds\n",
      "Epoch 40. Loss 5.0270. Elapsed 686 seconds\n",
      "Epoch 41. Loss 5.0235. Elapsed 702 seconds\n",
      "Epoch 42. Loss 5.0201. Elapsed 719 seconds\n",
      "Epoch 43. Loss 5.0168. Elapsed 736 seconds\n",
      "Epoch 44. Loss 5.0134. Elapsed 753 seconds\n",
      "Epoch 45. Loss 5.0104. Elapsed 769 seconds\n",
      "Epoch 46. Loss 5.0073. Elapsed 786 seconds\n",
      "Epoch 47. Loss 5.0044. Elapsed 804 seconds\n",
      "Epoch 48. Loss 5.0016. Elapsed 821 seconds\n",
      "Epoch 49. Loss 4.9987. Elapsed 838 seconds\n",
      "Epoch 50. Loss 4.9961. Elapsed 856 seconds\n",
      "Epoch 51. Loss 4.9934. Elapsed 873 seconds\n",
      "Epoch 52. Loss 4.9910. Elapsed 891 seconds\n",
      "Epoch 53. Loss 4.9884. Elapsed 908 seconds\n",
      "Epoch 54. Loss 4.9860. Elapsed 926 seconds\n",
      "Epoch 55. Loss 4.9836. Elapsed 944 seconds\n",
      "Epoch 56. Loss 4.9814. Elapsed 961 seconds\n",
      "Epoch 57. Loss 4.9791. Elapsed 978 seconds\n",
      "Epoch 58. Loss 4.9770. Elapsed 995 seconds\n",
      "Epoch 59. Loss 4.9748. Elapsed 1012 seconds\n",
      "Epoch 60. Loss 4.9727. Elapsed 1028 seconds\n",
      "Epoch 61. Loss 4.9707. Elapsed 1045 seconds\n",
      "Epoch 62. Loss 4.9687. Elapsed 1063 seconds\n",
      "Epoch 63. Loss 4.9667. Elapsed 1080 seconds\n",
      "Epoch 64. Loss 4.9648. Elapsed 1097 seconds\n",
      "Epoch 65. Loss 4.9629. Elapsed 1114 seconds\n",
      "Epoch 66. Loss 4.9610. Elapsed 1131 seconds\n",
      "Epoch 67. Loss 4.9593. Elapsed 1149 seconds\n",
      "Epoch 68. Loss 4.9575. Elapsed 1166 seconds\n",
      "Epoch 69. Loss 4.9559. Elapsed 1184 seconds\n",
      "Epoch 70. Loss 4.9542. Elapsed 1201 seconds\n",
      "Epoch 71. Loss 4.9525. Elapsed 1217 seconds\n",
      "Epoch 72. Loss 4.9510. Elapsed 1235 seconds\n",
      "Epoch 73. Loss 4.9493. Elapsed 1251 seconds\n",
      "Epoch 74. Loss 4.9478. Elapsed 1269 seconds\n",
      "Epoch 75. Loss 4.9462. Elapsed 1286 seconds\n",
      "Epoch 76. Loss 4.9448. Elapsed 1303 seconds\n",
      "Epoch 77. Loss 4.9433. Elapsed 1320 seconds\n",
      "Epoch 78. Loss 4.9419. Elapsed 1337 seconds\n",
      "Epoch 79. Loss 4.9405. Elapsed 1354 seconds\n",
      "Epoch 80. Loss 4.9391. Elapsed 1371 seconds\n",
      "Epoch 81. Loss 4.9377. Elapsed 1389 seconds\n",
      "Epoch 82. Loss 4.9363. Elapsed 1406 seconds\n",
      "Epoch 83. Loss 4.9350. Elapsed 1423 seconds\n",
      "Epoch 84. Loss 4.9338. Elapsed 1440 seconds\n",
      "Epoch 85. Loss 4.9325. Elapsed 1457 seconds\n",
      "Epoch 86. Loss 4.9312. Elapsed 1475 seconds\n",
      "Epoch 87. Loss 4.9300. Elapsed 1492 seconds\n",
      "Epoch 88. Loss 4.9288. Elapsed 1510 seconds\n",
      "Epoch 89. Loss 4.9275. Elapsed 1527 seconds\n",
      "Epoch 90. Loss 4.9264. Elapsed 1544 seconds\n",
      "Epoch 91. Loss 4.9252. Elapsed 1560 seconds\n",
      "Epoch 92. Loss 4.9241. Elapsed 1577 seconds\n",
      "Epoch 93. Loss 4.9230. Elapsed 1594 seconds\n",
      "Epoch 94. Loss 4.9219. Elapsed 1612 seconds\n",
      "Epoch 95. Loss 4.9208. Elapsed 1629 seconds\n",
      "Epoch 96. Loss 4.9198. Elapsed 1646 seconds\n",
      "Epoch 97. Loss 4.9187. Elapsed 1663 seconds\n",
      "Epoch 98. Loss 4.9176. Elapsed 1680 seconds\n",
      "Epoch 99. Loss 4.9165. Elapsed 1697 seconds\n",
      "Epoch 100. Loss 4.9155. Elapsed 1714 seconds\n",
      "Total time elapsed: 29 minutes\n"
     ]
    }
   ],
   "source": [
    "# TRAINING #\n",
    "tick = time.time()\n",
    "epoch_losses = []\n",
    "for epoch_num in range(1, num_epochs + 1):\n",
    "    batch_losses = []\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        ### YOUR CODE BELOW ###\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Extract the inputs and the targets\n",
    "        inputs, targets = batch\n",
    "        \n",
    "        # Transfer the inputs and the targets to GPUs, if available\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        # Run the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,targets)\n",
    "        \n",
    "        # Backpropagate the error\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append the loss\n",
    "        batch_losses.append(float(loss))\n",
    "        \n",
    "        ### YOUR CODE ABOVE ###\n",
    "        \n",
    "    epoch_loss = np.mean(np.array(batch_losses))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "    if epoch_num % report_every == 0:\n",
    "        tock = time.time()\n",
    "        print(\"Epoch {}. Loss {:.4f}. Elapsed {:.0f} seconds\".format(epoch_num, epoch_loss, tock-tick))\n",
    "\n",
    "print(\"Total time elapsed: {:.0f} minutes\".format((tock-tick)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, save the weights of the embedding layer by calling the `save_embeddings()` method. For consistency, we suggest that you name your embeddings file as `skipgram_trained.vec`, because we will use this file later in the homework. If you decide to stick with a different name, make sure to make changes to the code in the next part of the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results #\n",
    "### YOUR CODE BELOW ###\n",
    "model.save_embeddings(voc=voc, path = \"/usr/cs/undergrad/2021/cknopp/homework3/skipgram_trained.vec\")\n",
    "\n",
    "\n",
    "\n",
    "### YOUR CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the learning curve of your model using the `matplotlib` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8db688f15aee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# .. decreasing over time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m### YOUR CODE ABOVE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the learning curve\n",
    "### YOUR CODE BELOW ###\n",
    "\n",
    "# this normally will work, but it didn't run this time because I exited out after retraining the model and\n",
    "# ...saving the embeddings. However, I didn't run this cell before exiting out, so in order for this cell to run, I\n",
    "#.. have to retrain the dataset, which I don't have the time to do currently. But anyways, it shows a graph that is steadily\n",
    "# .. decreasing over time\n",
    "\n",
    "plt.plot(epoch_losses)\n",
    "\n",
    "### YOUR CODE ABOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you trained your own word embeddings, you will evaluate and compare them to the publicly available pre-trained embeddings released by Facebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Loading the data\n",
    "\n",
    "In this part of the homework we will use pre-trained word embeddings and explore their properties. In particular, we will use [fastText](https://fasttext.cc/) embeddings trained on the Wikipedia data. The easiest way to use pre-trained embeddings is to download them from the official website and use the [gensim](https://radimrehurek.com/gensim/) library. This library allows you to easily load pre-trained embeddings from several widely used formats (such as .txt or binary) and perform simple operations on the loaded vectors. Some of the commonly used functions are calculating the similarity between two tokens and finding tokens that are most similar to the given one.\n",
    "\n",
    "Recall that Jupyter notebook allows executing shell commands directly from within the notebook. Anything appearing after `!` on a line will be executed not by the Python kernel, but by the system shell. \n",
    "\n",
    "__Instructions__:\n",
    "\n",
    "- Execute the cells below to download the embeddings file and make sure it was unzipped to your directory without any errors.\n",
    "\n",
    "__Note__:\n",
    "- The downloaded archive will take up 650 MB of your disk space; the unzipped file requires additional 2.2 GB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘wiki-news-300d-1M.vec.zip’ already there; not retrieving.\n",
      "\n",
      "Archive:  wiki-news-300d-1M.vec.zip\n"
     ]
    }
   ],
   "source": [
    "!wget --no-clobber https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "!unzip -n wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 cknopp cs2021 2.2G Mar 14  2018 wiki-news-300d-1M.vec\r\n",
      "-rw------- 1 cknopp cs2021 651M Jan 18  2019 wiki-news-300d-1M.vec.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh wiki-news-300d-1M*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `.vec` file is just a text file in a special format. Namely, the first line contains the total number of tokens and the size (length) of the embeddings separted by a space. In our case the first line is `999994 300`, which means that this file contains embeddings for 999994 unique tokens and the every embeddings is a 300-dimensional vector.\n",
    "The following lines contain the embeddings themselves in the format `<word1>  <num_1> <num_2> ... <num_dimension>`. Feel free to open it in your favorite text editor and to see what it looks like.\n",
    "\n",
    "Now let's load the embeddings using the `gensim` library. It contains a convenient `KeyedVectors` class that provides basic methods for working with embeddings. See the [documentation](https://radimrehurek.com/gensim/models/keyedvectors.html) for details.\n",
    "After having loaded the embeddings you can perform different operations on them. In particular, you can obtain the embedding of a token through the indexing operation.\n",
    "\n",
    "__Instructions:__\n",
    "\n",
    "- Execute the cells below to see what the loaded vectors look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading vectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of the loaded embedding is (300,)\n",
      "The word 'cat' is represented by the following vector:\n",
      "[-1.339e-01  2.230e-02 -1.370e-02  2.932e-01  4.090e-02  5.750e-02\n",
      "  3.071e-01 -1.292e-01  2.580e-02  6.300e-03  5.870e-02 -5.850e-02\n",
      "  3.460e-02 -3.120e-02 -1.732e-01  3.100e-03  2.958e-01 -7.800e-02\n",
      " -1.700e-03 -1.413e-01 -1.614e-01 -1.175e-01 -1.075e-01  3.200e-02\n",
      " -1.782e-01 -1.970e-02 -6.160e-02  1.705e-01 -4.760e-02  7.560e-02\n",
      " -3.200e-03 -1.392e-01  2.340e-02  6.500e-02 -4.540e-02 -1.908e-01\n",
      "  9.710e-02  5.350e-02 -6.320e-02  3.850e-02 -9.830e-02  2.478e-01\n",
      " -3.940e-02 -8.490e-02 -7.500e-02  9.900e-03 -5.580e-02 -1.653e-01\n",
      " -9.400e-02  1.610e-02  5.890e-02 -9.200e-02 -6.695e-01 -1.776e-01\n",
      "  9.410e-02 -9.550e-02  2.543e-01  7.700e-02 -2.020e-02 -9.390e-02\n",
      "  1.804e-01 -2.320e-02 -9.140e-02 -2.394e-01 -9.150e-02 -4.700e-03\n",
      "  1.052e-01  6.460e-02 -1.324e-01  2.310e-02 -1.296e-01  7.170e-02\n",
      "  1.515e-01  1.323e-01  1.476e-01 -2.049e-01 -6.570e-02 -1.111e-01\n",
      " -1.320e-01 -8.600e-02 -4.190e-02  3.570e-02 -1.805e-01 -2.158e-01\n",
      " -3.400e-02  2.078e-01 -3.390e-02 -9.820e-02  2.527e-01 -1.490e-01\n",
      "  1.260e-02  1.510e-01 -2.436e-01 -1.652e-01  1.006e-01 -5.320e-02\n",
      " -1.454e-01 -1.740e-02 -1.036e-01  1.852e-01 -1.705e-01  2.480e-02\n",
      " -3.780e-02 -1.227e-01  5.900e-02  2.633e-01  4.900e-03 -4.090e-02\n",
      " -1.415e-01  6.120e-02  5.800e-02  5.620e-02  1.045e-01 -1.240e-02\n",
      " -4.440e-02  1.341e-01  1.711e-01 -9.940e-02 -3.307e-01 -3.143e-01\n",
      "  6.630e-02 -6.880e-02  1.416e-01 -7.550e-02  1.480e-02  1.561e-01\n",
      " -8.950e-02  6.260e-02 -2.570e-02  2.596e-01  1.730e-02 -1.139e-01\n",
      "  6.290e-02  2.720e-02 -1.486e-01  4.270e-02 -1.186e-01  7.990e-02\n",
      " -4.770e-02 -1.860e-02  6.410e-02  9.550e-02 -4.750e-02  2.931e-01\n",
      "  1.082e-01  2.561e-01  1.696e-01  1.640e-02 -6.090e-02  3.740e-02\n",
      " -9.220e-02  5.750e-02 -5.590e-02  4.350e-02 -5.600e-03 -1.070e-01\n",
      "  5.420e-02 -2.500e-02 -7.060e-02 -1.426e-01 -1.504e-01  8.150e-02\n",
      "  1.810e-02 -5.600e-03 -3.470e-02  2.369e-01  1.913e-01 -7.780e-02\n",
      "  2.890e-02  1.335e-01  1.270e-02 -1.221e-01 -1.583e-01  4.390e-02\n",
      "  1.477e-01 -4.140e-02  3.401e-01 -2.940e-02  6.750e-02 -1.123e-01\n",
      " -2.120e-02  2.717e-01 -1.052e-01 -1.587e-01 -3.400e-03 -9.250e-02\n",
      " -3.720e-02  4.090e-02 -1.972e-01  2.668e-01  1.400e-02 -3.290e-02\n",
      " -1.621e-01 -1.200e-03 -2.268e-01  1.295e-01  5.640e-02  4.520e-02\n",
      "  6.660e-02  2.400e-03  1.554e-01  6.810e-02 -1.028e-01 -2.160e-02\n",
      "  1.210e-02  1.119e-01 -1.279e-01 -1.766e-01 -4.390e-02 -1.222e-01\n",
      "  1.023e-01 -6.030e-02  1.101e-01 -1.405e-01 -1.890e-02  1.367e-01\n",
      " -9.380e-02 -6.190e-02 -6.190e-02 -1.338e-01 -4.080e-02 -5.320e-02\n",
      " -3.200e-02 -3.500e-02 -1.086e-01 -2.284e-01  7.640e-02 -5.190e-02\n",
      "  1.830e-02 -1.154e-01  2.030e-02  1.501e-01  3.600e-01 -1.229e-01\n",
      "  2.190e-02 -1.698e-01  8.880e-02 -7.160e-02 -2.248e-01 -1.023e-01\n",
      "  4.140e-02 -4.280e-02  1.703e-01 -4.590e-02 -7.970e-02 -2.310e-02\n",
      "  4.890e-02  2.904e-01 -1.348e-01  3.834e-01 -1.802e-01  1.307e-01\n",
      "  1.374e-01 -1.707e-01  2.280e-02 -7.490e-02  1.060e-02 -1.680e-02\n",
      " -6.000e-04 -1.122e-01  1.262e-01  1.386e-01  1.349e-01 -4.800e-03\n",
      " -4.114e-01 -2.500e-02 -4.230e-02  7.220e-02 -1.204e-01  1.058e-01\n",
      "  9.800e-03  1.647e-01  1.763e-01  9.230e-02 -2.770e-01  4.000e-02\n",
      "  6.360e-02  2.203e-01  1.034e-01  6.480e-02 -9.700e-03 -3.490e-02\n",
      "  8.580e-02  1.017e-01 -8.430e-02 -1.537e-01  3.210e-02 -1.021e-01\n",
      " -6.350e-02  1.438e-01 -6.670e-02 -6.500e-02  1.282e-01  2.044e-01\n",
      "  1.440e-01  7.570e-02 -3.420e-02 -1.519e-01  1.882e-01  4.850e-02]\n"
     ]
    }
   ],
   "source": [
    "# Extracting an embedding of a single word\n",
    "v1 = word_vectors['cat']\n",
    "print(\"The dimension of the loaded embedding is {}\".format(v1.shape))\n",
    "print(\"The word 'cat' is represented by the following vector:\")\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of  word embeddings properties\n",
    "In this step you will explore the properties of word embeddings and look at what operations you can perform on word vectors. \n",
    "\n",
    "__Instructions:__\n",
    "\n",
    "- Complete the code in the cells below by calling the appropriate methods of the `KeyedVectors` class and execute the cells to answer the questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Find top 3 most similar in terms of cosine similarity words and their similarity to the word `cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats: 0.855\n",
      "feline: 0.739\n",
      "kitten: 0.735\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "top3_most_similar = word_vectors.similar_by_word(\"cat\", 3)\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "for word, similarity in top3_most_similar:\n",
    "    print('{}: {:.3f}'.format(word, similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Find the most similar word to the word `good`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar word to the word \"good\" is the word \"bad\"\n",
      "The corresponding cosine similarity is 0.833\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "most_similar_to_good = word_vectors.similar_by_word(\"good\",3)[0]\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "print('The most similar word to the word \"good\" is the word \"{}\"'.format(most_similar_to_good[0]))\n",
    "print('The corresponding cosine similarity is {:.3f}'.format(most_similar_to_good[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think this happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this happened because these words can be used in the exact same syntactically manner and in the same contexts. The word can 'good' can be switched for 'bad' in any given context and the sentence will almost certainly still make sense, though it will not have the same meaning. So overall their vector representations are very similar because they are two highly related terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Find the cosine similarity between the words `woman` and `man`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between \"woman\" and \"man\" is 0.816\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "similarity = word_vectors.similarity('woman', 'man')\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "print('The cosine similarity between \"woman\" and \"man\" is {:.3f}'.format(similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Vector arithmetics\n",
    "The original word2vec paper illustrates the properties of word embeddings through showing examples of analogies. Perhaps the most famous example is the `king::man` <=> `queen::woman` analogy.\n",
    "To make sure this analogy holds true in the embedding space, one would need to do the following:\n",
    "\n",
    " 1. Get the vectors for the words \"woman\", \"king\", and \"man\"\n",
    " 2. Add the vectors for \"woman\" and \"king\" together\n",
    " 3. Substract the vector for the word \"man\" from the result\n",
    " 4. Find the word with the closest embedding (likely one will end up with the word \"queen\")\n",
    " \n",
    "In this question we will explore a similar example on verb tenses and how they are captured in the learned word embeddings. \n",
    "\n",
    "__Instructions:__\n",
    "- Print the the closest word to the vector $\\text{walking} + \\text{swam} - \\text{swimming}$.\n",
    "\n",
    "__Note__: use vector arithmetics operations (`+` and `-`) and the method `similar_by_vector` of the `KeyedVectors` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting word: walked\n",
      "Similarity: 0.816\n"
     ]
    }
   ],
   "source": [
    "v1 = word_vectors[\"walking\"]\n",
    "v2 = word_vectors[\"swam\"]\n",
    "v3 = word_vectors[\"swimming\"]\n",
    "\n",
    "### YOUR CODE BELOW ###\n",
    "v4_word = word_vectors.similar_by_vector((v1 + v2 - v3), 3)[0]\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "print('Resulting word: {}'.format(v4_word[0]))\n",
    "print('Similarity: {:.3f}'.format(v4_word[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing embeddings\n",
    "\n",
    "Using the same evaluation methods, you will now compare the embeddings you trained yourself with the pre-trained fastText embeddings. Execute the cell below to load the word vectors (change the names of the files if needed).\n",
    "\n",
    "__Note__:\n",
    "The execution of the cell below can take a while to finish (a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec', binary=False)\n",
    "skipgram = KeyedVectors.load_word2vec_format('skipgram_trained.vec', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Print out three most similar words (along with the similarities) to the word `money` for the both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model: \n",
      "  [('cash', 0.7241491079330444), ('moeny', 0.7205635905265808), ('funds', 0.7183815836906433)]\n",
      "Skip-gram model: \n",
      "  [('risk', 0.3182852864265442), ('closely', 0.31349098682403564), ('begin', 0.3111087381839752)]\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "top3_similar_fasttext = fasttext.similar_by_word(\"money\", 3)\n",
    "print(\"Pretrained model: \\n \", top3_similar_fasttext)\n",
    "top3_similar_skipgram = skipgram.similar_by_word(\"money\", 3)\n",
    "print(\"Skip-gram model: \\n \", top3_similar_skipgram)\n",
    "### YOUR CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 For the both models, compute the cosine similarities between the words `boy` and `man`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model: \n",
      "  0.68067825\n",
      "Skip-gram model: \n",
      "  0.051003225\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "fasttext_similarity = fasttext.similarity(\"boy\", \"man\")\n",
    "print(\"Pretrained model: \\n \", fasttext_similarity)\n",
    "skipgram_similarity = skipgram.similarity(\"boy\", \"man\")\n",
    "print(\"Skip-gram model: \\n \", skipgram_similarity)\n",
    "### YOUR CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Can you suggest at least four ways in which the performance of your skip-gram model can be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Larger corpus to train on \n",
    "\n",
    "2) Longer training time (i.e. higher number of epochs)\n",
    "\n",
    "3) Implement negative sampling in the model\n",
    "\n",
    "4) Adjust the learning rate, batch size, and skip_window until I find an optimal setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
